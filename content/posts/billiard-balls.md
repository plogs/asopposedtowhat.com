---
title: "Do billiard balls have free will?"
date: 2022-03-10T11:28:15-08:00
draft: false
---


## Billiard balls

Do billiard balls have free will?

If free will is "the power of acting without the constraint of
necessity or fate; the ability to act at one's own discretion," then,
no, billiard balls do not have free will.

Why? Because obviously when the cue ball strikes the eight ball, the
eight ball doesn't _decide_ to bounce left or right. The eight ball
doesn't decide anything at all. Its movement is wholly
constrained by physics.


## Self-driving cars

With more complexity comes more sophisticated behavior.

Self-driving cars can 'decide' for themselves when and
where to turn. The atoms in the microchips that run the software that
steers the vehicle are governed by physics, but in a loose sense the
car can act of its own accord.

More sophisticated behavior is not the same as free will, however.
The self-driving car does only what it was instructed to do.

## AI robots

What about some future artificial intelligence robot that mimics
human behavior so thoroughly that it's able to learn and behaves so
convincingly that it's able to pass the Turing Test?

For example, suppose that when traveling to the United Kingdom, the
robot figures out that it's best to drive on the left side of the
road and _chooses_ to drive on the other side, contrary to its original
programming in the United States.

Is that free will? Could it have decided differently?


## Free will

That's not free will in the sense defined above, because the robot is
behaving solely in accord with its programming. That is, it was
programmed to learn in that way. For example, if new danger X is
encountered, add it to the list of dangerous items and avoid X the
next time it's encountered.

Suppose we introduce a random number generator into the algorithm, so
that our human-like robot becomes unpredictable, much in the same way
that humans are unpredictable? Does that get us any closer to free
will (or a ghost in the machine)?

No, it only makes the machine's actions difficult if not impossible
to predict. We still wouldn't say that the machine _decided_ to
turn left instead of right.

## The test

Suppose we put two identical robots in the same situation at the same
time and at the same location and we watch how they behave. We can't
do this in real life, of course, because two physical objects can't
occupy the same space at the same time.

So suppose instead that we build two matrix-like worlds in
cyberspace, two identical simulations. And we set our robots loose in
the simulation at the same time and same place, etc. So that the worlds
are the same. Exactly the same.

If our robots are exactly the same, and they respond to the same
situation at the same time and place, could our robot (singular)
behave differently in cyberworld-one vs. cyberworld-two?

If so, what would account for the difference?

## The human upgrade

Suppose next that we enhance our robot in such a way that it's
self-reflecting. That is, suppose we add sub-routines to the robots
'brain' routine so that it's able to reflect upon itself, and reflect
upon how others react to it.

Eventually our robot may come to behave (`think', act) as if it's the
master of its own destiny. It may come to believe that _it's_
deciding, that _it's_ making decisions, that it _could_ have turned
right instead of left.


## Is it possible to have chosen otherwise?

If the answer is, "No, in identical worlds our robot would [must]
behave the same," then it has no free will in the sense
defined above. It could _not_ have chosen otherwise, even though it
'thinks' it could have.

Is that what you mean by 'free will' when you say that humans don't
have free will? Do you mean that even though it _feels_ like you
chose to read this, that in fact you had no real choice at all, no
more than the billiard ball could choose to go left instead of right?
